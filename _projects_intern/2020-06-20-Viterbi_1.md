---
title: "Visual Commonsense Reasoning"
collection: projects_intern
permalink: /projects_intern/2020-06-20-Viterbi_1
excerpt: 'Incorporating external common sense knowledge to existing visual linguistic encoders to perform more informed and valid inferences from images to solve downstream tasks like Visual Question Answering'
# date: 2009-10-01
# venue: 'International Conference on Pattern Recognition Applications and Methods 2019, Prague, Czech Republic'
# paperurl: 'http://www.insticc.org/Primoris/Resources/PaperPdf.ashx?idPaper=73925'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
group: "University of Southern California"
guide: "Prof Xiang Ren"
---
<!-- This paper is about the number 1. The number 2 is left for future work. -->

<!-- [Download paper here](http://www.insticc.org/Primoris/Resources/PaperPdf.ashx?idPaper=73925) -->

<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->

Visual Commonsense Reasoning is a Visual Question-Answering dataset that requires the system to make commomsense deductions from images to answer questions (and also choose a valid reasoning). Existing work use only complicated visual linguistic encoders to solve the task that are not very explainable. Several commonsense knowledge graphs exist that have been actively used in several question answering tasks. We incorporate scene information using Scene Graphs and Commonsense Information using Knowledge graph by extending the visula linguistic encoders like VL-BERT using Language Conditioned Graph Networks, Relation Networks and Multi-hop Graph Relation Networks.