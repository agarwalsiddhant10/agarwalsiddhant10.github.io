---
title: "Breaking Backdoored Classifiers"
collection: projects_intern
permalink: /projects_intern/2020-08-20-CMU
excerpt: 'A backdoored classifier cannot be attacked only by the adversary but can be broken by fundamentally anybody. Showed how robustified model of a poisoned classifier reveals its backdoors.'
# date: 2009-10-01
# venue: 'International Conference on Pattern Recognition Applications and Methods 2019, Prague, Czech Republic'
# paperurl: 'http://www.insticc.org/Primoris/Resources/PaperPdf.ashx?idPaper=73925'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
group: "Carnegie Mellon University"
guide: "Prof Zico Kolter"
---
<!-- This paper is about the number 1. The number 2 is left for future work. -->

<!-- [Download paper here](http://www.insticc.org/Primoris/Resources/PaperPdf.ashx?idPaper=73925) -->

<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->

A backdoored classifier is a poisoned classifier where the adversary can make it always classify images to a target class by simply adding a trigger to the image at test time. Such a classifier is generated if the adversary can access a small sample of training data using which it can insert the backdoor into the classifier. It is generally thought that a backdoored classifier can be attacked only by the adversary as he only has access to the trigger. We show that a backdoored classifier is fundamentally broken and backdoors can be generated by anyone which will have a attack rate similar to the original trigger or sometimes even better. We create a robustified version of the poisoned classifier using a certifiable robust technique and use the perceptual property of robust classifiers to extract backdoors from the adversarial examples of the robustified model. Also used deep dream and regularization techniques (Tikhonov regularization and Total Variation loss) to further enhance the adversarial examples and extract backdoor better.

Submitted in ICLR 2021 conference. (Preprint coming soon).