
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  .link2 {
    text-decoration: none;
    display: inline;
    margin-right: 5px;
  }

  .fakelink {
    text-decoration: none;
    /* cursor: pointer; */
  }

  element.style {
    overflow: hidden;
    display: block;
  }
  .pre-white-space {
    white-space: pre;
  }
  .bibref {
    margin-top: 10px;
    margin-left: 10px;
    display: none;
    font-size: 14px;
    font-family: monospace;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  code {
  font-family: Consolas,"courier new";
  color: black;
  background-color: #f1f1f1;
  padding: 2px;
  font-size: 99%;
  white-space: pre;
}
</style>
<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <head>
    <link rel="icon" type="image/png" href="resources/ucsd_logo.png">
    <title>"f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences"</title>
    <meta property='og:title' content='f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences' />
    <meta property="og:description" content="Siddhant Agarwal, Ishan Durugkar, Peter Stone, Amy Zhang. f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences." />
    <meta property='og:url' content='https://agarwalsiddhant10/projects/fpg.html' />
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <!-- <script type="text/javascript"
      src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script> -->
    <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
  </head>
  <body>
        <center><span style="font-size:40px;font-weight:bold;color:#182B49">f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences</span></center>

        <table align=center width=900px>
          <tr>
            <td align=center width=180px>
            <center><span style="font-size:20px"><a href="https://agarwalsiddhant10.github.io/" target="_blank">Siddhant Agarwal<sup>1</sup></a></span></center></td>
            <td align=center width=180px>
              <center><span style="font-size:20px"><a href="https://idurugkar.github.io/" target="_blank">Ishan Durugkar<sup>2</sup></a></span></center></td>
            <td align=center width=180px>
                <center><span style="font-size:20px"><a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone<sup>1, 2</sup></a></span></center></td>
            <td align=center width=180px>
                <center><span style="font-size:20px"><a href="https://amyzhang.github.io/" target="_blank">Amy Zhang<sup>1</sup></a></span></center></td>
            <tr/>
         </table>

        <!-- <center><span style="font-size:15px;color:#000000">&#8224: Equal Contribution</span></center> -->

        <table align=center>
          <tr>
            <td align=center style="white-space:nowrap;padding:10px"><center><sup>1 </sup><span style="font-size:18px">The University of Texas at Austin</span></center></td>
            <td align=center style="white-space:nowrap;padding:10px"><center><sup>2 </sup><span style="font-size:18px">Sony AI</span></center></td>
          <tr/>
        </table> 
        <table align=center width=600px>
          <tr>
            <td align=center width=600px>
            <center><span style="font-size:24px"><a href="https://neurips.cc/Conferences/2023" target="_blank">Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2023</a> </span></center></td>
          <tr/>
        </table>
        <table align=center>
          <tr>
            <td align=center style="white-space:nowrap;padding:10px"><center><span style="font-size:18px"><a href="https://arxiv.org/abs/2310.06794v1">Paper</a></span></center></td>
            <td align=center style="white-space:nowrap;padding:10px"><center><span style="font-size:18px"><a href="https://github.com/agarwalsiddhant10/f-pg">Code</a></span></center></td>
            <td align=center style="white-space:nowrap;padding:10px"><center><span style="font-size:18px"><a href="#">Presentation (Coming Soon)</a></span></center></td>
            <td align=center style="white-space:nowrap;padding:10px"><center><span style="font-size:18px"><a href="#">Slides (Coming Soon)</a></span></center></td>
          <tr/>
        </table> 
        <!-- <table align=center width=200px>
            <tr><td width=200px>
              <center><a href="images/bpr.png"><img src = "images/bpr.png" width="900" height="300"></img></a><br></center>
            </td></tr>
        </table> -->

        <center id="abstract"><h1>Abstract</h1></center>
        Goal-Conditioned Reinforcement Learning (RL) problems often have access to sparse rewards where the agent receives a reward signal only when it has achieved the goal, making policy optimization a difficult problem.
  Several works augment this sparse reward with a learned dense reward function, but this can lead to sub-optimal policies if the reward is misaligned. 
  Moreover, recent works have demonstrated that effective shaping rewards for a particular problem can depend on the underlying learning algorithm. 
  This paper introduces a novel way to encourage exploration called
  $f$-Policy Gradients, or $f$-PG. $f$-PG minimizes the f-divergence between the agent's state visitation distribution and the goal, which we show can lead to an optimal policy. We derive gradients for various f-divergences to optimize this objective. 
  Our learning paradigm provides dense learning signals for exploration in sparse reward settings. We further introduce an entropy-regularized policy optimization objective, that we call $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective. 
  We show that several metric-based shaping rewards like L2 can be used with $s$-MaxEnt RL, providing a common ground to study such metric-based shaping rewards with efficient exploration. We find that $f$-PG has better performance compared to standard policy gradient methods on a challenging gridworld as well as the Point Maze and FetchReach environments.<br>
        <hr>

        <h2>Introduction</h2>
        Goal Conditioned Reinforcement Learning requires being able to learn from spare rewards. 
        Prior works have used a learnt reward function to augment the sparse reward but these can lead to suboptimal policies if the rewards are misaligned to the goal.
        Divergence minimization has been extensively studied in imitation learning but its use in RL has been limited.
        The commonly used imiatation learning methods that aim to minimize some form of divergence between the agent's visitation distribution and an expert's visitation distribution construct a minmax objective as a lower bound to the divergence.
        Moreover, they use a discriminator to construct the reward function which is non-stationary.
        We propose a novel framework, $f$-Policy Gradients or $f$-PG, that minimizes the $f$-divergence between the agent's state visitation distribution and the goal using analytical gradients.
        We also show that special cases of our objective can be shown to optimize a reward (can also be a metric-based shaping reward) along with the entropy of the state-visitation distribution introducing the $state$-MaxEnt RL objective.

        <h2>The $f$-PG objective</h2>
        The agents learn by minimizing the following  $f$-divergence:
        <br><br>
        <center>$J(\theta) = D_f(p_\theta(s) || p_g(s))$</center>
        <br>
        where $p_\theta(s)$ is the agent's state visitation distribution and $p_g(s)$ is the goal distribution. We can derive the analytical gradient for the objective which looks similar to policy gradients.
        <br><br>
        <center><img src="fpg/gradient.png" width="700" height="100"></img></center>
        <br>

        <h2>$state$-MaxEnt RL</h2>

        We present the following Lemma which states that special case of $f$-PG ($f(u) = u\log{u}$), the agent maximizes a reward of $\log{p_g(s)}$ along with the entropy of the state visitation distribution.
        This is different from the commonly studied MaxEnt RL (which will will call $\pi$-MaxEnt RL) which maximizes the entropy of the policy.
        <br><br>
        <center><img src="fpg/fkl.png" width="700" height="50"></img></center>
        <br>
        Consider a gridworld where the agent start and goal distributions are seperated by a wall, making the agent necessary to travel around the wall to reach the goal.
        The exploration of the $\pi$-MaxEnt RL and $s$-MaxEnt RL agents vary as shown below (the evolution of the state-visitation distributions)
        <br>
        <table align="center" width="800px">
        <tr>
          <td align=center style="padding:20px;width:350px;vertical-align:middle">
            <img src="fpg/softq_vis.gif" width=300px></img>
            <br>
            <center>$\pi$-MaxEnt RL</center></td>
            </td>
            <td align=center style="padding:20px;width:350px;vertical-align:middle">
             <img src="fpg/fkl_vis.gif" width=300px></img>
              <br>
              <center>$s$-MaxEnt RL</center></td>
            </td>
          <tr/>
       </table>

        <h2>Learning Signals</h2>
        $f$-PG involves a learning signal $f'(\frac{p_\theta(s)}{p_g(s)})$ to weigh the log probabilities of the policy. 
        It is thus important to understand how $f'(\frac{p_\theta(s)}{p_g(s)})$ behaves for goal-conditioned RL settings. 
        During the initial stages of training, the agent visits regions with very low $p_g$. For such states, the signal has a lower value than the states that have lower $p_\theta$, i.e., the unexplored states. 
        This is because for any convex function $f$, $f'(x)$ is an increasing function, so minimizing $f'(\frac{p_\theta(s)}{p_g(s)})$ (recall that we are minimizing $f$-divergence) will imply minimizing $p_\theta(s)$ for the states with low $p_g(s)$. 
        The only way to do this is to increase the entropy of the state-visitation distribution, directly making the agent explore new states. 
        As long as there is no significant overlap between the two distributions, it will push $p_\theta$ down to a flatter distribution until there is enough overlap with the goal distribution when it will pull back the agent's visitation again to be closer to the goal distribution.
        <br><br><br>
        <table align="center" width="1100px">
          <tr height="205px">
            <td valign="middle" style="padding:10px;width=50px;">
              <p style="transform:rotate(-90.0deg)">$f'\big(\frac{p_\theta(s)}{p_g(s)}\big)$</p>
            </td>
            <td rowspan="2" align=center style="padding:10px;width:250px;vertical-align:middle">
              <img src="fpg/fkl_reach_vis.gif" width=240px></img>
              <br>
              <center>Forward KL</center></td>
              </td>
              <td rowspan="2" align=center style="padding:10px;width:250px;vertical-align:middle">
               <img src="fpg/rkl_reach_vis.gif" width=240px></img>
                <br>
                <center>Reverse KL</center></td>
              </td>
              <td rowspan="2" align=center style="padding:10px;width:250px;vertical-align:middle">
                <img src="fpg/js_reach_vis.gif" width=240px></img>
                 <br>
                 <center>Jenson Shanon</center></td>
               </td>
               <td rowspan="2" align=center style="padding:10px;width:250px;vertical-align:middle">
                <img src="fpg/chi2_reach_vis.gif" width=240px></img>
                 <br>
                 <center>$\chi^2$</center>
                </td>
              </tr>
            <tr height="195px">
            <td valign="middle" style="padding:10px;width=50px;">
              <p style="transform:rotate(-90.0deg)">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp$p_\theta(s)$</p>
            </td>
            <!-- <td align="center" style="padding:10px;width=50px;transform:rotate(-90.0deg)">
              $f'\big(\frac{p_\theta(s)}{p_g(s)}\big)$
            </td> -->
            <!-- <td align="center" style="padding:10px;width=50px;transform:rotate(-90.0deg)">
              $f'\big(\frac{p_\theta(s)}{p_g(s)}\big)$
            </td>
            <td align="center" style="padding:10px;width=50px;transform:rotate(-90.0deg)">
              $f'\big(\frac{p_\theta(s)}{p_g(s)}\big)$
            </td>
            <td align="center" style="padding:10px;width=50px;transform:rotate(-90.0deg)">
              $f'\big(\frac{p_\theta(s)}{p_g(s)}\big)$
            </td> -->
            </tr>
         </table>

        <h2>Results</h2>

        We compare $f$-PG with several previous works that have used distribution matching to provide some shaped rewards like AIM, GAIL, AIRL etc. 
        We initially perform experiments on a gridworld followed by Point Maze and FetchReach environments.
        
        <h3>Gridworld</h3>
        Here, the baselines are implemented on top of Soft Q Learning which is a $\pi$-MaxEnt RL algorithm. 
        <table align="center" width="1000px">
          <tr>
            <td align=center style="padding:10px;width:320px;vertical-align:middle">
              <img src="fpg/fkl_grid.jpg" width=300px></img>
              <br>
              <center>$fkl$-PG</center></td>
              </td>
              <td align=center style="padding:10px;width:320px;vertical-align:middle">
               <img src="fpg/rkl_grid.jpg" width=300px></img>
                <br>
                <center>$rkl$-PG</center></td>
              </td>
              <td align=center style="padding:10px;width:320px;vertical-align:middle">
                <img src="fpg/aim.jpg" width=300px></img>
                 <br>
                 <center>AIM</center></td>
               </td>
              </tr>

              <tr>
                <td align=center style="padding:10px;width:320px;vertical-align:middle">
                  <img src="fpg/gail.jpg" width=300px></img>
                  <br>
                  <center>GAIL</center></td>
                  </td>
                  <td align=center style="padding:10px;width:320px;vertical-align:middle">
                   <img src="fpg/airl.jpg" width=300px></img>
                    <br>
                    <center>AIRL</center></td>
                  </td>
                  <td align=center style="padding:10px;width:320px;vertical-align:middle">
                    <img src="fpg/fairl.jpg" width=300px></img>
                     <br>
                     <center>FAIRL</center></td>
                   </td>
                  </tr>
                </table>

          <h3>Point Maze and FetchReach</h3>
          In these experiments, the baselines are implemented on top of an on-policy PPO to provide a fair comparison with the sample complexity.

          <table align="center" width="1000px">
            <tr>
              <td align=center style="padding:10px;width:900px;vertical-align:middle">
                <img src="fpg/pointmaze.png" width=900px></img>
              </td>
            </tr>
            <tr>
              <td align=center style="padding:10px;width:900px;vertical-align:middle">
                <img src="fpg/pointmaze_fetch.png" width=900px></img>
              </td>
            </tr>
          </table>

      <hr>
      <code background-color="#f1f1f1">@inproceedings{agarwal2023fpg,
          &nbsp;&nbsp; author = {Agarwal, Siddhant and Durugkar, Ishan and Stone, Peter and Zhang, Amy},
          &nbsp;&nbsp; booktitle = {Advances in Neural Information Processing Systems},
          &nbsp;&nbsp; title = {$f$ Policy Gradients: A General Framework for Goal Conditioned RL using $f$-Divergences},
          &nbsp;&nbsp; volume = {36}, 
          &nbsp;&nbsp; year = {2023}
         }</code>

      <br/>

    <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>