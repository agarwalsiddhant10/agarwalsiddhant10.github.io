
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  .link2 {
    text-decoration: none;
    display: inline;
    margin-right: 5px;
  }

  .fakelink {
    text-decoration: none;
    /* cursor: pointer; */
  }

  element.style {
    overflow: hidden;
    display: block;
  }
  .pre-white-space {
    white-space: pre;
  }
  .bibref {
    margin-top: 10px;
    margin-left: 10px;
    display: none;
    font-size: 14px;
    font-family: monospace;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <head>
    <link rel="icon" type="image/png" href="resources/ucsd_logo.png">
    <title>"f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences"</title>
    <meta property='og:title' content='f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences' />
    <meta property="og:description" content="Siddhant Agarwal, Ishan Durugkar, Peter Stone, Amy Zhang. f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences." />
    <meta property='og:url' content='https://agarwalsiddhant10/projects/fpg.html' />
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
      src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>
  <body>
        <center><span style="font-size:40px;font-weight:bold;color:#182B49">f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences</span></center>

        <table align=center width=900px>
          <tr>
            <td align=center width=180px>
            <center><span style="font-size:20px"><a href="https://agarwalsiddhant10.github.io/" target="_blank">Siddhant Agarwal<sup>1</sup></a></span></center></td>
            <td align=center width=180px>
              <center><span style="font-size:20px"><a href="#" target="_blank">Ishan Durugkar<sup>2</sup></a></span></center></td>
            <td align=center width=180px>
                <center><span style="font-size:20px"><a href="#" target="_blank">Peter Stone<sup>1, 2</sup></a></span></center></td>
            <td align=center width=180px>
                <center><span style="font-size:20px"><a href="#" target="_blank">Amy Zhang<sup>1</sup></a></span></center></td>
            <tr/>
         </table>

        <!-- <center><span style="font-size:15px;color:#000000">&#8224: Equal Contribution</span></center> -->

        <table align=center width=600px>
          <tr>
            <td align=center width=300px><center><sup>1 </sup><span style="font-size:18px">The University of Texas at Austin</span></center></td>
            <td align=center width=300px><center><sup>2 </sup><span style="font-size:18px">Sony AI</span></center></td>
          <tr/>
        </table> 
        <table align=center width=600px>
          <tr>
            <td align=center width=600px>
            <center><span style="font-size:24px"><a href="https://neurips.cc/Conferences/2023" target="_blank">Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2023</a> </span></center></td>
          <tr/>
        </table>
        <!-- <table align=center width=200px>
            <tr><td width=200px>
              <center><a href="images/bpr.png"><img src = "images/bpr.png" width="900" height="300"></img></a><br></center>
            </td></tr>
        </table> -->

        <center id="abstract"><h1>Abstract</h1></center>
        Goal-Conditioned Reinforcement Learning (RL) problems often have access to sparse rewards where the agent receives a reward signal only when it has achieved the goal, making policy optimization a difficult problem.
  Several works augment this sparse reward with a learned dense reward function, but this can lead to sub-optimal policies if the reward is misaligned. 
  Moreover, recent works have demonstrated that effective shaping rewards for a particular problem can depend on the underlying learning algorithm. 
  This paper introduces a novel way to encourage exploration called
  $f$-Policy Gradients, or $f$-PG. $f$-PG minimizes the f-divergence between the agent's state visitation distribution and the goal, which we show can lead to an optimal policy. We derive gradients for various f-divergences to optimize this objective. 
  Our learning paradigm provides dense learning signals for exploration in sparse reward settings. We further introduce an entropy-regularized policy optimization objective, that we call $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective. 
  We show that several metric-based shaping rewards like L2 can be used with $s$-MaxEnt RL, providing a common ground to study such metric-based shaping rewards with efficient exploration. We find that $f$-PG has better performance compared to standard policy gradient methods on a challenging gridworld as well as the Point Maze and FetchReach environments.<br>
        <hr>

        <h2>Introduction</h2>
        Goal Conditioned Reinforcement Learning requires being able to learn from spare rewards. 
        Prior works have used a learnt reward function to augment the sparse reward but these can lead to suboptimal policies if the rewards are misaligned to the goal.
        Divergence minimization has been extensively studied in imitation learning but its use in RL has been limited.
        The commonly used imiatation learning methods that aim to minimize some form of divergence between the agent's visitation distribution and an expert's visitation distribution construct a minmax objective as a lower bound to the divergence.
        Moreover, they use a discriminator to construct the reward function which is non-stationary.
        We propose a novel framework, $f$-Policy Gradients or $f$-PG, that minimizes the $f$-divergence between the agent's state visitation distribution and the goal using analytical gradients.
        We also show that special cases of our objective can be shown to optimize a reward (can also be a metric-based shaping reward) along with the entropy of the state-visitation distribution introducing the $state$-MaxEnt RL objective.

        <h2>The $f$-PG objective</h2>
        The agents learn by minimizing the following  $f$-divergence:
        <br><br>
        <center>$J(\theta) = D_f(p_\theta(s) || p_g(s))$</center>
        <br>
        where $p_\theta(s)$ is the agent's state visitation distribution and $p_g(s)$ is the goal distribution. We can derive the analytical gradient for the objective which looks similar to policy gradients.
        <br><br>
        <center><img src="fpg/gradient.png" width="700" height="100"></img></center>
        <br>

        <h2>$state$-MaxEnt RL</h2>

        We present the following Lemma which states that special case of $f$-PG ($f(u) = u\log{u}$), the agent maximizes a reward of $\log{p_g(s)}$ along with the entropy of the state visitation distribution.
        This is different from the commonly studied MaxEnt RL (which will will call $\pi$-MaxEnt RL) which maximizes the entropy of the policy.
        <br><br>
        <center><img src="fpg/fkl.png" width="700" height="50"></img></center>
        <br>
        Consider a gridworld where the agent start and goal distributions are seperated by a wall, making the agent necessary to travel around the wall to reach the goal.
        The exploration of the $\pi$-MaxEnt RL and $s$-MaxEnt RL agents vary as shown below (the evolution of the state-visitation distributions)

        <h2>Learning Signals</h2>
        $f$-PG involves a learning signal $f'(\frac{p_\theta(s)}{p_g(s)})$ to weigh the log probabilities of the policy. 
        It is thus important to understand how $f'(\frac{p_\theta(s)}{p_g(s)})$ behaves for goal-conditioned RL settings. 
        During the initial stages of training, the agent visits regions with very low $p_g$. For such states, the signal has a lower value than the states that have lower $p_\theta$, i.e., the unexplored states. 
        This is because for any convex function $f$, $f'(x)$ is an increasing function, so minimizing $f'(\frac{p_\theta(s)}{p_g(s)})$ (recall that we are minimizing $f$-divergence) will imply minimizing $p_\theta(s)$ for the states with low $p_g(s)$. 
        The only way to do this is to increase the entropy of the state-visitation distribution, directly making the agent explore new states. 
        As long as there is no significant overlap between the two distributions, it will push $p_\theta$ down to a flatter distribution until there is enough overlap with the goal distribution when it will pull back the agent's visitation again to be closer to the goal distribution.
        <center id="sourceCode"><h1>Paper & Code</h1></center>


        <table align=center width=900px>
            <tr></tr>
          <tr>
            <!-- <td >
        <a href="#"><img class="paperpreview" src="images/bpr.png" width="250px"/></a>
          </td> -->
          <!-- <td></td> -->
          <td width=1000px > <span style="font-size:20px">
            Siddhant Agarwal, Ishan Durugkar, Peter Stone, Amy Zhang<br/>
              <a href="#">
                f-Policy Gradients: A General Framework for Goal Conditioned RL using f-Divergences</a> <br/> <i>37<sup>th</sup>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2023 <br/>
            [<a href="#">PDF</a>]
            [<a href="#">Video Presentation</a>]
            [<a href="#">Poster</a>]
              [<a href="#">Slides</a>]


</span>
        </td>
        </tr>

      </table>

      <br>
      <hr>

      <br/>

    <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>