<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Siddhant Agarwal</title>
  
  <meta name="author" content="Siddhant Agarwal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/siddhant.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Siddhant Agarwal</name>
              </p>
              <p>I am a third year Ph.D. student in the Computer Science Department at University of Texas at Austin, working with <a href="https://amyzhang.github.io/">Prof. Amy Zhang</a> and <a href="https://www.cs.utexas.edu/~pstone/index.shtml">Prof. Peter Stone</a>. 
                I am interested in utilizing reward-free transitions for improving reinforcement learning be it through designing principled representation learning techniques or downstream algorithms independent of rewards. 
                For this I have been looking at RL from the perspective of visitation distributions and utilizing unsupervised RL, goal-conditioned RL and distribution matching learning techniques. 
                Previously, I completed my Dual Degree (B.Tech. + M.Tech.) in Computer Science and Engineering from the Indian Institute of Technology Kharagpur with a department rank 1. 
                During my undergrad, I was a leading member of the Autonomous Ground Vehicles research group where I worked on a number of perception and planning projects for autonomous driving.
                I completed my bachelor thesis on "Reinforcement Explanation Learning" under the supervision of <a href="https://cse.iitkgp.ac.in/~adas/">Prof. Abir Das</a>. 
                <!-- My research interests include goal conditioned reinforcement learning and representation learning.  -->
                <!-- I like to view reinforcement learning from state-visitation distributions perspective and look to extend RL as a distribution matching problem. I am also interested in principled algoritms for learning state abstractions or representations that are useful for unknown downstream tasks. -->
                <!-- I have spent time at the Autonomous Ground Vehicle Research Group at IIT Kharagpur. I was also a part of the Computer Vision and Intelligence Research Lab where I worked with <a href="https://cse.iitkgp.ac.in/~adas/">Prof. Abir Das</a> on RL and explainable AI for two years. I have interned under <a href="https://zicokolter.com/">Prof. Zico Kolter</a> where I worked on adversarial robustness. I have also worked on the conjugation on knowledge graph reasoning and reinforcement learning with <a href="https://shanzhenren.github.io/">Prof. Xiang Ren</a>. I worked with <a href="https://agarwl.github.io/">Rishabh Agarwal</a> and <a href="https://mila.quebec/en/person/aaron-courville/">Prof. Aaron Courville</a> on generalization in reinforcement learning in 2021. -->

              </p>
              <p>
                <center><a href="https://scholar.google.com/citations?user=2OhzYF8AAAAJ&hl=en">Google Scholar</a> / <a href="https://github.com/agarwalsiddhant10">Github</a> / <a href="https://twitter.com/agsidd10">Twitter</a> / <a href="https://www.linkedin.com/in/agsidd10">LinkedIn</a>/ <a href="mailto:agarwalsiddhant10@gmail.com">Email</a> / <a href="images/cv.pdf">Resume</a></center>
              </p>
              <p style="text-align:center">

              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <a href="images/siddhant.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/siddhant.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr> <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                  <li> [Sept 2023]: Paper accepted at <a href="https://neurips.cc/Conferences/2023">NeurIPS 2023</a>.</li>
                  <li> [Aug 2022]: Joined Ph.D. program in the Computer Science Department at the University of Texas at Austin</li>
                  <li> [May 2022]: Completed my Dual Degree in Computer Science at IIT Kharagpur, securing rank 1 in the department</li>
                  <li> [Oct 2021]: Paper accepted at <a href="https://sites.google.com/view/deep-rl-workshop-neurips2021">NeurIPS 2021 workshop on Deep RL</a> and <a href="https://sites.google.com/view/ecorl2021/">Ecological Theory of RL</a>. <br>
                  <li> [Oct 2021]: Paper accepted at the <a href="https://xai4debugging.github.io/">NeurIPS 2021 workshop on eXplainable AI approaches for debugging and diagnosis</a>.<br>
                  <li> [July 2021]: Interned at Goldman Sachs <br>
                  <li> [March 2021]: Paper accepted at the <a href="https://aisecure-workshop.github.io/aml-iclr2021/">ICLR 2021 workshop on Security and Safety in Machine Learning systems</a>.<br>
                  <li> [Jan 2021]: Paper accepted at <a href="https://iclr.cc/Conferences/2021">ICLR 2021</a>.<br>
                  <li> [Nov 2020]: Paper accepted and nominated for best paper at <a href="https://kr2ml.github.io/2020/">KR2ML workshop, NeurIPS 2020</a>.<br>
              </ul>
            </td> </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My primary research interests are reinforcement learning and robot learning. I aim to develop effective da  ta-driven RL algorithms. I want to work at the intersection of offline RL and model-based RL in the areas of generalisability of algorithms and address safety in exploration.

              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <!-- Papers list -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="images/rlzero.png" width="160" height="100">
           </td>
           <td width="75%" valign="middle">
             <a href="https://arxiv.org/abs/2412.05718l">
               <papertitle>RLZero: Zero Shot Language to Behaviors without Any Supervision</papertitle>
             </a>
             <br> Harshit Sikchi*, Siddhant Agarwal*, Pranaya Jajoo*, Samyak Parajuli*, Max Rudolph*, Peter Stone, Amy Zhang, Scott Niekum
               <br><i>* denotes equal contribution</i><br>
             <em>Under Submission ICML 2025</em><br>
             <a href="https://hari-sikchi.github.io/rlzero/">project page</a> /
             <a href="https://arxiv.org/abs/2412.05718">paper</a> 
             <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
             <p> We introduce a framework for unsupervised RL to produce optimal policies for any language instruction.</p>
           </td>
         </tr> </tbody></table>

         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="images/psm.png" width="160" height="100">
           </td>
           <td width="75%" valign="middle">
             <a href="https://agarwalsiddhant10.github.io/projects/psm.html">
               <papertitle>Proto Successor Measure: Representing the Behavior Space of an RL Agent</papertitle>
             </a>
             <br> Siddhant Agarwal*, Harshit Sikchi*, Peter Stone, Amy Zhang
               <br><i>* denotes equal contribution</i><br>
             <em>Under Submission ICML 2025</em><br>
             <a href="https://agarwalsiddhant10.github.io/projects/psm.html">project page</a> /
             <a href="https://arxiv.org/abs/2411.19418">paper</a> 
             <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
             <!-- <a href="https://github.com/agarwalsiddhant10/f-pg">code</a> -->
             <p> We provide a representation that can produce optimal policies for any reward function in the environment.</p>
           </td>
         </tr> </tbody></table>

         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="images/robocup.png" width="160" height="100">
           </td>
           <td width="75%" valign="middle">
             <a href="https://agarwalsiddhant10.github.io/index.html">
               <papertitle>Reinforcement Learning Within the Classical Robotics Stack: A Case Study in Robot Soccer</papertitle>
             </a>
             <br> Adam Labiosa∗, Zhihan Wang∗, Siddhant Agarwal, William Cong, Geethika Hemkumar, Abhinav Narayan Harish,Benjamin Hong, Josh Kelle, Chen Li, Yuhao Li, Zisen Shao, Peter Stone, Josiah P. Hanna
               <br><i>* denotes equal contribution, authors in alphabetical order</i><br>
             <em>ICRA 2025</em><br>
             <a href="https://agarwalsiddhant10.github.io/index.html">project page</a> /
             <a href="https://arxiv.org/abs/2412.09417">paper</a> /
             <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
             <a href="https://github.com/wistex-united/wistex-system-2024">code</a>
             <p> The Robotics stack used by our RoboCup Standard Platform League that used RL to train high level abstracted robotics behavior. </p>
           </td>
         </tr> </tbody></table>

         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="images/airhockey.png" width="160" height="100">
           </td>
           <td width="75%" valign="middle">
             <a href="https://rlairhockey.github.io">
               <papertitle>Robot Air Hockey: A Manipulation Testbed for Robot Learning with Reinforcement Learning</papertitle>
             </a>
             <br> Caleb Chuck∗, Carl Qi∗, Michael J. Munje∗, Shuozhe Li∗, Max Rudolph∗, Chang Shi∗, Siddhant Agarwal∗,
             Harshit Sikchi∗, Abhinav Peri, Sarthak Dayal, Evan Kuo, Kavan Mehta, Anthony Wang, Peter Stone,
             Amy Zhang, Scott Niekum
               <br><i>* denotes equal contribution</i><br>
             <em>CRA Workshop Manipulation Skills, 2024</em><br>
             <a href="https://rlairhockey.github.io">project page</a> /
             <a href="https://arxiv.org/pdf/2405.03113">paper</a> /
             <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
             <a href="https://github.com/YOUR%20REPO%20HERE">code</a>
             <p> A two stage simulator and real-world manipulator system designed as a air-hockey for to study effects of RL and IL. </p>
           </td>
         </tr> </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fpg.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://agarwalsiddhant10.github.io/projects/fpg.html">
                <papertitle>f-Policy Gradients: A General Fr  amework for Goal Conditioned RL using f-Divergences</papertitle>
              </a>
              <br> Siddhant Agarwal, Ishan Durugkar, Peter Stone, Amy Zhang
                <br>
              <em>NeurIPS 2023</em><br>
              <a href="https://agarwalsiddhant10.github.io/projects/fpg.html">project page</a> /
              <a href="https://arxiv.org/abs/2310.06794v1">arXiv</a> /
              <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
              <a href="https://github.com/agarwalsiddhant10/f-pg">code</a>
              <p> We introduce a general framework for goal conditioned RL using f-divergences.</p>
            </td>
          </tr> </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bpr.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://agarwalsiddhant10.github.io/projects/bpr.html">
                <papertitle>Behavior Predictive Representations for Generalization in Reinforcement Learning</papertitle>
              </a>
              <br> Siddhant Agarwal, Aaron Courville, Rishabh Agarwal
                <br>
              <em>NeurIPS 2021 workshop on Deep Reinforcement Learning and Ecological Theory of RL</em><br>
              <a href="https://agarwalsiddhant10.github.io/projects/bpr.html">project page</a> /
              <a href="https://openreview.net/forum?id=b5PJaxS6Jxg">paper</a> /
              <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
              <!-- <a href="agarwalsiddhant10.github.io/">code</a> -->
              <p> We introduce latent representations that can predict the behavior of the agent at future steps to improve generalization in RL.</p>
            </td>
          </tr> </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <img src="images/rexl.jpg" width="160" height="100">
             </td>
             <td width="75%" valign="middle">
               <a href="https://agarwalsiddhant10.github.io/projects/rexl.html">
                 <papertitle>Reinforcement Explanation Learning</papertitle>
               </a>
               <br>
                 Siddhant Agarwal, Owais Iqbal, Sree Aditya Buridi, Mada Manjusha, Abir Das<br>
               <em>NeurIPS 2021 workshop on eXplainable AI approaches for debugging and diagnosis</em><br>
               <a href="https://agarwalsiddhant10.github.io/projects/rexl.html">project page</a> /
               <a href="https://arxiv.org/abs/2111.13406">arXiv</a> /
               <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
               <a href="https://github.com/CVIR/RExL">code</a>
               <p> We reformulate the process of generating saliency maps using perturbation based methods for black box models as a Markov Decsion Process and use RL to optimally search for the best saliency map, thereby reducing the inference time without hurting the performance.</p>
             </td>
           </tr> </tbody></table>

           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <img src="images/poisoned_classifier.png" width="160" height="100">
             </td>
             <td width="75%" valign="middle">
               <a href="https://agarwalsiddhant10.github.io/projects/poisoned_classifier.html">
                 <papertitle>Poisoned Classifiers are not only backdoored, they are fundamentally broken</papertitle>
               </a>
               <br>
                 Minjie Sun, Siddhant Agarwal, Zico Kolter<br>
               <em>ICLR 2021 workshop on Security and Safety in Machine Learning systems.</em><br>
               <a href="https://agarwalsiddhant10.github.io/projects/poisoned_classifier.html">project page</a> /
               <a href="https://arxiv.org/abs/2010.09080">arXiv</a> /
               <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
               <a href="https://github.com/locuslab/breaking-poisoned-classifier">code</a>
               <p> We show that backdoored classifiers can be attacked by anyone rather than only the adversary. We propose an attack that generates alternate triggers for the poisoned classifiers.</p>
             </td>
           </tr> </tbody></table>

           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <img src="images/kg.png" width="160" height="100">
             </td>
             <td width="75%" valign="middle">
               <a href="https://agarwalsiddhant10.github.io/projects/deceive_kg.html">
                 <papertitle>Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbations</papertitle>
               </a>
               <br>
                 Mrigank Raman, Aaron Chan*, Siddhant Agarwal*, Peifeng Wang, Hansen Wang, Sungchul Kim, Ryan Rossi, Handong Zhao, Nedim Lipka, Xiang Ren<br>
                 <i>* denotes equal contribution</i><br>
               <em>International Conference of Learned Representations</em> 2021 and <em> NeurIPS 2020 workshop on Knowledge Representation and Reasoning in Machine Learning [Best paper nomination]</em><br>
               <a href="https://agarwalsiddhant10.github.io/projects/deceive_kg.html">project page</a> /
               <a href="https://arxiv.org/abs/2010.12872">arXiv</a> /
               <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
               <a href="https://github.com/INK-USC/deceive-KG-models.git">code</a>
               <p> We show that using reinforcement learning (or even simple heuristics) we can produce deceptively perturbed knowledge graphs that preserve the downstream performance of the kg-aumented models. </p>
             </td>
           </tr> </tbody></table>

           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
             <td style="padding:20px;width:25%;vertical-align:middle">
               <img src="images/traffic_sign_classification.png" width="160" height="100">
             </td>
             <td width="75%" valign="middle">
               <a href="https://agarwalsiddhant10.github.io/projects/traffic_sign_classification.html">
                 <papertitle>Traffic Sign Classification using HOG-SURF features and Convolutional Neural Networks</papertitle>
               </a>
               <br>
                 Rishabh Madan*, Deepank Agrawal*, Shreyas Kowshik*, Harsh Maheshwari*, Siddhant Agarwal*, Debashish Chakravarty<br>
                 <i>* denotes equal contribution, dice roll</i><br>
               <em>International Conference on Pattern Recognition Application and Methods</em>, Prague, 2019<br>
               <a href="https://agarwalsiddhant10.github.io/projects/traffic_sign_classification.html">project page</a> /
               <a href="https://www.scitepress.org/Papers/2019/73925/73925.pdf">paper</a> /
               <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
               <!-- <a href="https://agarwalsiddhant10.github.io/">code</a> -->
               <p> We use a hybrid CNN archticture that uses two image processing features to classify the images. The CNN architecture has significantly less number of parameters than any of the state of the art methods on GTSRB. </p>
             </td>
           </tr> </tbody></table>

           <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="images/agv.png" width="160" height="100">
           </td>
           <td width="75%" valign="middle">
             <a href="https://agarwalsiddhant10.github.io/projects/agv.html">
               <papertitle>Autonomous Ground Vehicles Research Group</papertitle>
             </a>
             <em>IIT Kharagpur</em><br>
             <a href="https://agarwalsiddhant10.github.io/projects/agv.html">project page</a> /
             <!-- <a href="https://agarwalsiddhant10.github.io/">arXiv</a> / -->
             <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
             <!-- <a href="https://agarwalsiddhant10.github.io/">code</a> -->
             <p> We worked to develop vision, planning and localization modules for a category 4 autonomous vehicle. </p>
           </td>
         </tr> </tbody></table>

         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="images/cluster_management.png" width="160" height="100">
           </td>
           <td width="75%" valign="middle">
             <a href="https://agarwalsiddhant10.github.io/projects/cluster_management_system.html">
               <papertitle>Cluster Management System</papertitle>
             </a>
             <em>Distribution Systems Project</em><br>
             <a href="https://agarwalsiddhant10.github.io/projects/cluster_management_system.html">project page</a> /
             <a href="https://agarwalsiddhant10.github.io/projects/images/Cluster_Middleware.pdf">report</a> /
             <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
             <a href="https://github.com/agarwalsiddhant10/cluster-middleware">code</a>
             <p> We investigate commonly used cluster management systems like SLURM and Condor. We further develop a fault-tolerant active-passive SLURM-like cluster management system.  </p>
           </td>
         </tr> </tbody></table>

         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="projects/images/graph.png" width="160" height="100">
           </td>
           <td width="75%" valign="middle">
             <a href="https://agarwalsiddhant10.github.io/projects/cuda_programming_project.html">
               <papertitle>Accelerating Graph Algorithms using GPU</papertitle>
             </a>
             <em>CUDA Programming Project</em><br>
             <a href="https://agarwalsiddhant10.github.io/projects/cuda_programming_project.html">project page</a> /
             <a href="https://agarwalsiddhant10.github.io/projects/images/HP3.pdf">report</a> /
             <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
             <a href="https://github.com/TheLethalCode/HP3_project">code</a>
             <p> We use parallization in a GPU to accelarate graph algorithms like BFS, DFS and Single Source Shortest Path and All Pair Shortest Path to achieve massive speedups. </p>
           </td>
         </tr> </tbody></table>

         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <img src="projects/images/chatbot.png" width="160" height="100">
           </td>
           <td width="75%" valign="middle">
             <a href="https://agarwalsiddhant10.github.io/projects/jarvic.html">
               <papertitle>Just A Rather Very Interesting Chatbot</papertitle>
             </a>
             <em>Software Engineering Project</em><br>
             <a href="https://agarwalsiddhant10.github.io/projects/jarvic.html">project page</a> /
             <a href="https://agarwalsiddhant10.github.io/projects/images/J.A.R.V.I.C..pdf">presentation</a> /
             <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
             <a href="https://github.com/agarwalsiddhant10/J.A.R.V.I.C.">code</a>
             <p> We develop an android chatbot application using a trained LSTM based encoder-decoder module trained for emotion specific chats. We use a simple classifier to choose the appropriate model from the chat. </p>
           </td>
         </tr> </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Webpage template courtesy: <a href="https://jonbarron.info/">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
