---
title: "Reinforcement Learning based Local Planner"
collection: projects_intern
permalink: /projects_intern/2020-08-18-RL
excerpt: 'Using only the Lidar input and localization, learnt policies that provide the optimal velocities to a vehicle. Providing velocities rather than the actuator torques help the RL algorithm to be independent of the underlying vehicle dynamics.'
# date: 2009-10-01
# venue: 'International Conference on Pattern Recognition Applications and Methods 2019, Prague, Czech Republic'
# paperurl: 'http://www.insticc.org/Primoris/Resources/PaperPdf.ashx?idPaper=73925'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
group: "Autonomous Ground Vehicles Research Group"
# guide: "Prof Zico Kolter"
---
<!-- This paper is about the number 1. The number 2 is left for future work. -->

<!-- [Download paper here](http://www.insticc.org/Primoris/Resources/PaperPdf.ashx?idPaper=73925) -->

<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->

The plan was to learn policies that can be transfered from one vehicle to the other easily. So if the policies controlled the actuators directly, then policies would not be easily transferable. So instead we learn policies to predict velocities of the vehicle at each step. We assume that there is an underlying controller that is tuned for the vehicle that generates actuator outputs directly from the target velocities. If the controller is tuned aptly, we can say that the lag between the actual velocity and target velocity predicted by the policy will be more or less same for any device and that is the dynamics that we use. 